  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   894  100   894    0     0   2134      0 --:--:-- --:--:-- --:--:--  2133
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   274  100   274    0     0    826      0 --:--:-- --:--:-- --:--:--   827
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0 3254k    0 31937    0     0  68003      0  0:00:49 --:--:--  0:00:49 67951 91 3254k   91 2987k    0     0  2081k      0  0:00:01  0:00:01 --:--:-- 2081k100 3254k  100 3254k    0     0  2179k      0  0:00:01  0:00:01 --:--:-- 2178k
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   644    0   644    0     0   1202      0 --:--:-- --:--:-- --:--:--  1203
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0  149M    0  185k    0     0  94787      0  0:27:35  0:00:02  0:27:33  174k  0  149M    0  985k    0     0   327k      0  0:07:47  0:00:03  0:07:44  476k  1  149M    1 2226k    0     0   554k      0  0:04:36  0:00:04  0:04:32  724k  2  149M    2 3466k    0     0   690k      0  0:03:41  0:00:05  0:03:36  850k  2  149M    2 4231k    0     0   713k      0  0:03:34  0:00:05  0:03:29  847k  3  149M    3 5541k    0     0   798k      0  0:03:11  0:00:06  0:03:05 1085k  4  149M    4 6611k    0     0   832k      0  0:03:04  0:00:07  0:02:57 1140k  4  149M    4 7597k    0     0   849k      0  0:03:00  0:00:08  0:02:52 1088k  5  149M    5 8516k    0     0   855k      0  0:02:59  0:00:09  0:02:50 1023k  6  149M    6 9586k    0     0   874k      0  0:02:55  0:00:10  0:02:45 1065k  6  149M    6 10.2M    0     0   881k      0  0:02:53  0:00:11  0:02:42  995k  7  149M    7 11.2M    0     0   887k      0  0:02:52  0:00:12  0:02:40  974k  8  149M    8 12.0M    0     0   880k      0  0:02:53  0:00:13  0:02:40  937k  8  149M    8 12.8M    0     0   876k      0  0:02:54  0:00:14  0:02:40  916k  9  149M    9 13.5M    0     0   866k      0  0:02:56  0:00:15  0:02:41  849k  9  149M    9 14.1M    0     0   854k      0  0:02:59  0:00:16  0:02:43  791k  9  149M    9 14.8M    0     0   844k      0  0:03:01  0:00:17  0:02:44  733k 10  149M   10 15.4M    0     0   834k      0  0:03:03  0:00:18  0:02:45  707k 10  149M   10 16.1M    0     0   825k      0  0:03:05  0:00:20  0:02:45  676k 11  149M   11 16.7M    0     0   817k      0  0:03:07  0:00:21  0:02:46  659k 11  149M   11 17.5M    0     0   814k      0  0:03:08  0:00:22  0:02:46  679k 12  149M   12 18.1M    0     0   807k      0  0:03:09  0:00:23  0:02:46  676k 12  149M   12 18.7M    0     0   802k      0  0:03:10  0:00:23  0:02:47  678k 12  149M   12 19.3M    0     0   796k      0  0:03:12  0:00:24  0:02:48  675k 13  149M   13 20.0M    0     0   791k      0  0:03:13  0:00:25  0:02:48  682k 13  149M   13 20.8M    0     0   794k      0  0:03:12  0:00:26  0:02:46  702k 14  149M   14 21.6M    0     0   794k      0  0:03:12  0:00:27  0:02:45  730k 14  149M   14 22.4M    0     0   793k      0  0:03:13  0:00:28  0:02:45  751k 15  149M   15 23.2M    0     0   793k      0  0:03:13  0:00:29  0:02:44  778k 15  149M   15 23.9M    0     0   790k      0  0:03:13  0:00:30  0:02:43  785k 16  149M   16 24.5M    0     0   787k      0  0:03:14  0:00:31  0:02:43  751k 16  149M   16 25.3M    0     0   788k      0  0:03:14  0:00:32  0:02:42  754k 17  149M   17 26.1M    0     0   787k      0  0:03:14  0:00:33  0:02:41  751k 17  149M   17 26.8M    0     0   787k      0  0:03:14  0:00:34  0:02:40  751k 18  149M   18 27.6M    0     0   785k      0  0:03:14  0:00:35  0:02:39  757k 19  149M   19 28.4M    0     0   787k      0  0:03:14  0:00:36  0:02:38  788k 19  149M   19 29.2M    0     0   787k      0  0:03:14  0:00:37  0:02:37  785k 20  149M   20 29.9M    0     0   786k      0  0:03:14  0:00:38  0:02:36  781k 20  149M   20 30.6M    0     0   786k      0  0:03:14  0:00:39  0:02:35  783k 21  149M   21 31.4M    0     0   786k      0  0:03:14  0:00:40  0:02:34  791k 21  149M   21 32.2M    0     0   786k      0  0:03:14  0:00:41  0:02:33  778k 22  149M   22 32.9M    0     0   786k      0  0:03:14  0:00:42  0:02:32  775k 22  149M   22 33.8M    0     0   788k      0  0:03:14  0:00:43  0:02:31  799k 23  149M   23 34.6M    0     0   790k      0  0:03:13  0:00:44  0:02:29  817k 23  149M   23 35.8M    0     0   798k      0  0:03:11  0:00:45  0:02:26  894k 24  149M   24 36.8M    0     0   803k      0  0:03:10  0:00:46  0:02:24  941k 25  149M   25 37.9M    0     0   809k      0  0:03:09  0:00:47  0:02:22 1007k 26  149M   26 38.9M    0     0   814k      0  0:03:08  0:00:48  0:02:20 1041k 26  149M   26 39.8M    0     0   816k      0  0:03:07  0:00:49  0:02:18 1055k 27  149M   27 40.9M    0     0   822k      0  0:03:06  0:00:50  0:02:16 1038k 28  149M   28 42.0M    0     0   828k      0  0:03:04  0:00:51  0:02:13 1066k 28  149M   28 43.0M    0     0   833k      0  0:03:03  0:00:52  0:02:11 1065k 29  149M   29 44.1M    0     0   837k      0  0:03:02  0:00:53  0:02:09 1069k 30  149M   30 45.2M    0     0   842k      0  0:03:01  0:00:54  0:02:07 1103k 30  149M   30 46.3M    0     0   847k      0  0:03:00  0:00:55  0:02:05 1107k 31  149M   31 47.3M    0     0   852k      0  0:02:59  0:00:56  0:02:03 1098k 32  149M   32 48.4M    0     0   856k      0  0:02:58  0:00:57  0:02:01 1099k 33  149M   33 49.4M    0     0   859k      0  0:02:58  0:00:58  0:02:00 1092k 34  149M   34 50.9M    0     0   871k      0  0:02:55  0:00:59  0:01:56 1184k 34  149M   34 52.3M    0     0   879k      0  0:02:54  0:01:00  0:01:54 1234k 35  149M   35 53.8M    0     0   890k      0  0:02:52  0:01:01  0:01:51 1325k 36  149M   36 55.3M    0     0   900k      0  0:02:50  0:01:02  0:01:48 1409k 37  149M   37 56.7M    0     0   908k      0  0:02:48  0:01:03  0:01:45 1486k 39  149M   39 58.5M    0     0   923k      0  0:02:45  0:01:04  0:01:41 1547k 40  149M   40 60.3M    0     0   936k      0  0:02:43  0:01:05  0:01:38 1631k 41  149M   41 61.9M    0     0   947k      0  0:02:41  0:01:06  0:01:35 1660k 42  149M   42 63.7M    0     0   959k      0  0:02:39  0:01:07  0:01:32 1704k 43  149M   43 65.4M    0     0   971k      0  0:02:37  0:01:08  0:01:29 1777k 44  149M   44 67.0M    0     0   980k      0  0:02:36  0:01:09  0:01:27 1725k 45  149M   45 68.6M    0     0   990k      0  0:02:34  0:01:10  0:01:24 1700k 47  149M   47 70.3M    0     0  1001k      0  0:02:33  0:01:11  0:01:22 1722k 48  149M   48 72.0M    0     0  1011k      0  0:02:31  0:01:12  0:01:19 1722k 49  149M   49 73.6M    0     0  1019k      0  0:02:30  0:01:13  0:01:17 1681k 50  149M   50 74.8M    0     0  1022k      0  0:02:29  0:01:14  0:01:15 1602k 50  149M   50 76.2M    0     0  1027k      0  0:02:29  0:01:15  0:01:14 1546k 51  149M   51 77.5M    0     0  1031k      0  0:02:28  0:01:17  0:01:11 1459k 52  149M   52 78.8M    0     0  1036k      0  0:02:27  0:01:17  0:01:10 1396k 53  149M   53 80.7M    0     0  1047k      0  0:02:26  0:01:18  0:01:08 1461k 55  149M   55 82.8M    0     0  1061k      0  0:02:24  0:01:19  0:01:05 1654k 56  149M   56 85.0M    0     0  1075k      0  0:02:22  0:01:20  0:01:02 1819k 58  149M   58 87.0M    0     0  1088k      0  0:02:20  0:01:21  0:00:59 1975k 59  149M   59 89.1M    0     0  1100k      0  0:02:19  0:01:22  0:00:57 2091k 60  149M   60 91.0M    0     0  1110k      0  0:02:18  0:01:23  0:00:55 2102k 62  149M   62 93.3M    0     0  1125k      0  0:02:16  0:01:24  0:00:52 2145k 63  149M   63 95.6M    0     0  1139k      0  0:02:14  0:01:25  0:00:49 2172k 65  149M   65 97.9M    0     0  1153k      0  0:02:12  0:01:26  0:00:46 2219k 67  149M   67  100M    0     0  1168k      0  0:02:11  0:01:27  0:00:44 2295k 68  149M   68  102M    0     0  1178k      0  0:02:10  0:01:28  0:00:42 2321k 70  149M   70  104M    0     0  1194k      0  0:02:08  0:01:29  0:00:39 2366k 71  149M   71  107M    0     0  1208k      0  0:02:06  0:01:30  0:00:36 2400k 73  149M   73  109M    0     0  1221k      0  0:02:05  0:01:31  0:00:34 2405k 74  149M   74  111M    0     0  1232k      0  0:02:04  0:01:32  0:00:32 2351k 76  149M   76  114M    0     0  1246k      0  0:02:02  0:01:33  0:00:29 2446k 78  149M   78  116M    0     0  1260k      0  0:02:01  0:01:34  0:00:27 2452k 79  149M   79  119M    0     0  1271k      0  0:02:00  0:01:35  0:00:25 2411k 81  149M   81  121M    0     0  1285k      0  0:01:59  0:01:36  0:00:23 2449k 83  149M   83  124M    0     0  1299k      0  0:01:57  0:01:37  0:00:20 2551k 85  149M   85  127M    0     0  1322k      0  0:01:55  0:01:38  0:00:17 2766k 88  149M   88  131M    0     0  1351k      0  0:01:53  0:01:39  0:00:14 3058k 90  149M   90  134M    0     0  1368k      0  0:01:51  0:01:40  0:00:11 3224k 91  149M   91  137M    0     0  1378k      0  0:01:51  0:01:41  0:00:10 3182k 93  149M   93  139M    0     0  1388k      0  0:01:50  0:01:42  0:00:08 3130k 95  149M   95  142M    0     0  1404k      0  0:01:49  0:01:43  0:00:06 3018k 97  149M   97  145M    0     0  1417k      0  0:01:48  0:01:44  0:00:04 2745k 99  149M   99  148M    0     0  1431k      0  0:01:47  0:01:45  0:00:02 2694k100  149M  100  149M    0     0  1437k      0  0:01:46  0:01:46 --:--:-- 2716k
17/08/11 07:02:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/11 07:02:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
put: `/griffin/data/enum-profiling-sample/test_src.avro': File exists
17/08/11 07:02:54 INFO profile.ProfileApplication: ===============begin================
17/08/11 07:02:54 INFO batch.Application$: [Ljava.lang.String;@5bda8e08
17/08/11 07:02:54 INFO batch.Application$: env.json
17/08/11 07:02:54 INFO batch.Application$: {"name":"profileEnum_1502434974596_White","type":"profile","source":{"type":"avro","version":"1.7","config":{"file.name":"hdfs:///griffin/data/enum-profiling-sample/test_src.avro"}},"evaluateRule":{"sampleRatio":1,"rules":"$source.race == 'White'"}}
17/08/11 07:02:55 INFO batch.Application$: params validation pass
17/08/11 07:02:55 INFO spark.SparkContext: Running Spark version 1.6.0
17/08/11 07:02:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/08/11 07:02:55 WARN spark.SparkConf: 
SPARK_JAVA_OPTS was detected (set to '-Dspark.driver.port=53411').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with conf/spark-defaults.conf to set defaults for an application
 - ./spark-submit with --driver-java-options to set -X options for a driver
 - spark.executor.extraJavaOptions to set -X options for executors
 - SPARK_DAEMON_JAVA_OPTS to set java options for standalone daemons (master or worker)
        
17/08/11 07:02:55 WARN spark.SparkConf: Setting 'spark.executor.extraJavaOptions' to '-Dspark.driver.port=53411' as a work-around.
17/08/11 07:02:55 WARN spark.SparkConf: Setting 'spark.driver.extraJavaOptions' to '-Dspark.driver.port=53411' as a work-around.
17/08/11 07:02:55 INFO spark.SecurityManager: Changing view acls to: root
17/08/11 07:02:55 INFO spark.SecurityManager: Changing modify acls to: root
17/08/11 07:02:55 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/08/11 07:02:56 WARN util.Utils: Service 'sparkDriver' could not bind on port 53411. Attempting port 53412.
17/08/11 07:02:56 WARN util.Utils: Service 'sparkDriver' could not bind on port 53412. Attempting port 53413.
17/08/11 07:02:56 INFO util.Utils: Successfully started service 'sparkDriver' on port 53413.
17/08/11 07:02:56 INFO slf4j.Slf4jLogger: Slf4jLogger started
17/08/11 07:02:56 INFO Remoting: Starting remoting
17/08/11 07:02:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:53414]
17/08/11 07:02:57 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 53414.
17/08/11 07:02:57 INFO spark.SparkEnv: Registering MapOutputTracker
17/08/11 07:02:57 INFO spark.SparkEnv: Registering BlockManagerMaster
17/08/11 07:02:57 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c23cc30e-7b20-4381-9518-a067aff988ff
17/08/11 07:02:57 INFO storage.MemoryStore: MemoryStore started with capacity 511.1 MB
17/08/11 07:02:57 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/08/11 07:02:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
17/08/11 07:02:57 WARN component.AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1964)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1955)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo$$anonfun$run$1.apply$mcV$sp(BatchProfileAlgo.scala:46)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo$$anonfun$run$1.apply(BatchProfileAlgo.scala:42)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo$$anonfun$run$1.apply(BatchProfileAlgo.scala:42)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo.run(BatchProfileAlgo.scala:42)
	at org.apache.griffin.measure.batch.Application$.main(Application.scala:92)
	at org.apache.griffin.measure.batch.Application.main(Application.scala)
	at com.griffin.profile.profiling.EnumProfiling.<init>(EnumProfiling.java:71)
	at com.griffin.profile.ProfileApplication.main(ProfileApplication.java:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/08/11 07:02:57 WARN component.AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@1706a5c9: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:414)
	at sun.nio.ch.Net.bind(Net.java:406)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:252)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:262)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1964)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1955)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:262)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:136)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at org.apache.spark.SparkContext$$anonfun$13.apply(SparkContext.scala:481)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:481)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo$$anonfun$run$1.apply$mcV$sp(BatchProfileAlgo.scala:46)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo$$anonfun$run$1.apply(BatchProfileAlgo.scala:42)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo$$anonfun$run$1.apply(BatchProfileAlgo.scala:42)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.griffin.measure.batch.algo.BatchProfileAlgo.run(BatchProfileAlgo.scala:42)
	at org.apache.griffin.measure.batch.Application$.main(Application.scala:92)
	at org.apache.griffin.measure.batch.Application.main(Application.scala)
	at com.griffin.profile.profiling.EnumProfiling.<init>(EnumProfiling.java:71)
	at com.griffin.profile.ProfileApplication.main(ProfileApplication.java:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
17/08/11 07:02:57 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
17/08/11 07:02:57 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/08/11 07:02:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
17/08/11 07:02:57 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4041
17/08/11 07:02:57 INFO util.Utils: Successfully started service 'SparkUI' on port 4041.
17/08/11 07:02:57 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4041
17/08/11 07:02:57 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-fdc0e06a-59fa-4def-934a-64db298d9738/httpd-9db89619-26e8-4b21-8c23-ab9ded152665
17/08/11 07:02:57 INFO spark.HttpServer: Starting HTTP Server
17/08/11 07:02:57 INFO server.Server: jetty-8.y.z-SNAPSHOT
17/08/11 07:02:57 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:53347
17/08/11 07:02:57 INFO util.Utils: Successfully started service 'HTTP file server' on port 53347.
17/08/11 07:02:58 INFO spark.SparkContext: Added JAR file:/root/griffin/enum-profiling/griffin-profile-1.0-SNAPSHOT-jar-with-dependencies.jar at http://172.17.0.2:53347/jars/griffin-profile-1.0-SNAPSHOT-jar-with-dependencies.jar with timestamp 1502434978045
17/08/11 07:02:58 INFO client.RMProxy: Connecting to ResourceManager at sandbox/172.17.0.2:8032
17/08/11 07:02:58 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers
17/08/11 07:02:58 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
17/08/11 07:02:58 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
17/08/11 07:02:58 INFO yarn.Client: Setting up container launch context for our AM
17/08/11 07:02:58 INFO yarn.Client: Setting up the launch environment for our AM container
17/08/11 07:02:58 INFO yarn.Client: Preparing resources for our AM container
17/08/11 07:02:58 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/home/spark_lib/spark-assembly-1.6.0-hadoop2.6.0.jar
17/08/11 07:02:58 INFO yarn.Client: Uploading resource file:/apache/spark/conf/hive-site.xml -> hdfs://sandbox:9000/user/root/.sparkStaging/application_1500434006041_0076/hive-site.xml
17/08/11 07:02:59 INFO yarn.Client: Uploading resource file:/tmp/spark-fdc0e06a-59fa-4def-934a-64db298d9738/__spark_conf__4683509044915200000.zip -> hdfs://sandbox:9000/user/root/.sparkStaging/application_1500434006041_0076/__spark_conf__4683509044915200000.zip
17/08/11 07:02:59 INFO spark.SecurityManager: Changing view acls to: root
17/08/11 07:02:59 INFO spark.SecurityManager: Changing modify acls to: root
17/08/11 07:02:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/08/11 07:02:59 INFO yarn.Client: Submitting application 76 to ResourceManager
17/08/11 07:02:59 INFO impl.YarnClientImpl: Submitted application application_1500434006041_0076
17/08/11 07:03:00 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:00 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1502434979188
	 final status: UNDEFINED
	 tracking URL: http://sandbox:8088/proxy/application_1500434006041_0076/
	 user: root
17/08/11 07:03:01 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:02 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:03 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:04 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:05 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:06 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:07 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:08 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:09 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:10 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:11 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:12 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:13 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:14 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:15 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:16 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:17 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:18 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:19 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:20 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:21 INFO yarn.Client: Application report for application_1500434006041_0076 (state: ACCEPTED)
17/08/11 07:03:21 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)
17/08/11 07:03:21 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> sandbox, PROXY_URI_BASES -> http://sandbox:8088/proxy/application_1500434006041_0076), /proxy/application_1500434006041_0076
17/08/11 07:03:21 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
17/08/11 07:03:22 INFO yarn.Client: Application report for application_1500434006041_0076 (state: RUNNING)
17/08/11 07:03:22 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 172.17.0.2
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1502434979188
	 final status: UNDEFINED
	 tracking URL: http://sandbox:8088/proxy/application_1500434006041_0076/
	 user: root
17/08/11 07:03:22 INFO cluster.YarnClientSchedulerBackend: Application application_1500434006041_0076 has started running.
17/08/11 07:03:22 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51832.
17/08/11 07:03:22 INFO netty.NettyBlockTransferService: Server created on 51832
17/08/11 07:03:22 INFO storage.BlockManagerMaster: Trying to register BlockManager
17/08/11 07:03:22 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:51832 with 511.1 MB RAM, BlockManagerId(driver, 172.17.0.2, 51832)
17/08/11 07:03:22 INFO storage.BlockManagerMaster: Registered BlockManager
17/08/11 07:03:28 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)
17/08/11 07:03:29 INFO hive.HiveContext: Initializing execution hive, version 1.2.1
17/08/11 07:03:29 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
17/08/11 07:03:29 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/08/11 07:03:30 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (sandbox:46620) with ID 1
17/08/11 07:03:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager sandbox:42164 with 511.1 MB RAM, BlockManagerId(1, sandbox, 42164)
17/08/11 07:03:30 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/08/11 07:03:30 INFO metastore.ObjectStore: ObjectStore, initialize called
17/08/11 07:03:30 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/08/11 07:03:30 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/08/11 07:03:32 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/08/11 07:03:32 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/08/11 07:03:32 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/08/11 07:03:34 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/08/11 07:03:34 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/08/11 07:03:34 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/08/11 07:03:34 INFO metastore.ObjectStore: Initialized ObjectStore
17/08/11 07:03:34 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/08/11 07:03:34 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
17/08/11 07:03:34 INFO metastore.HiveMetaStore: Added admin role in metastore
17/08/11 07:03:34 INFO metastore.HiveMetaStore: Added public role in metastore
17/08/11 07:03:34 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
17/08/11 07:03:35 INFO metastore.HiveMetaStore: 0: get_all_databases
17/08/11 07:03:35 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
17/08/11 07:03:35 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
17/08/11 07:03:35 INFO HiveMetaStore.audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/08/11 07:03:35 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/08/11 07:03:35 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/3bab2fe3-b24c-4520-b6b8-aca299c10b8e
17/08/11 07:03:35 INFO session.SessionState: Created local directory: /tmp/root/3bab2fe3-b24c-4520-b6b8-aca299c10b8e
17/08/11 07:03:35 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/3bab2fe3-b24c-4520-b6b8-aca299c10b8e/_tmp_space.db
17/08/11 07:03:35 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
17/08/11 07:03:35 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/08/11 07:03:35 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
17/08/11 07:03:35 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/08/11 07:03:35 INFO hive.metastore: Trying to connect to metastore with URI thrift://sandbox:9083
17/08/11 07:03:35 INFO hive.metastore: Connected to metastore.
17/08/11 07:03:36 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/be2727b3-9cff-4189-b0fc-44a3d72bbf60
17/08/11 07:03:36 INFO session.SessionState: Created local directory: /tmp/root/be2727b3-9cff-4189-b0fc-44a3d72bbf60
17/08/11 07:03:36 INFO session.SessionState: Created HDFS directory: /tmp/hive-root/root/be2727b3-9cff-4189-b0fc-44a3d72bbf60/_tmp_space.db
17/08/11 07:03:36 INFO persist.LoggerPersist: profileEnum_1502434974596_White start
17/08/11 07:03:36 INFO avro.AvroRelation: Listing hdfs://sandbox:9000/griffin/data/enum-profiling-sample/test_src.avro on driver
17/08/11 07:03:37 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 61.9 KB, free 61.9 KB)
17/08/11 07:03:37 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.6 KB, free 81.5 KB)
17/08/11 07:03:37 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.2:51832 (size: 19.6 KB, free: 511.1 MB)
17/08/11 07:03:37 INFO spark.SparkContext: Created broadcast 0 from flatMap at AvroDataConnector.scala:66
17/08/11 07:03:37 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 212.9 KB, free 294.3 KB)
17/08/11 07:03:37 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.6 KB, free 313.9 KB)
17/08/11 07:03:37 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.2:51832 (size: 19.6 KB, free: 511.1 MB)
17/08/11 07:03:37 INFO spark.SparkContext: Created broadcast 1 from hadoopFile at AvroRelation.scala:121
17/08/11 07:03:37 INFO mapred.FileInputFormat: Total input paths to process : 1
17/08/11 07:03:37 INFO spark.SparkContext: Starting job: count at ProfileCore.scala:42
17/08/11 07:03:37 INFO scheduler.DAGScheduler: Got job 0 (count at ProfileCore.scala:42) with 2 output partitions
17/08/11 07:03:37 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (count at ProfileCore.scala:42)
17/08/11 07:03:37 INFO scheduler.DAGScheduler: Parents of final stage: List()
17/08/11 07:03:37 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/11 07:03:37 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at map at ProfileCore.scala:36), which has no missing parents
17/08/11 07:03:37 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.3 KB, free 327.2 KB)
17/08/11 07:03:37 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.2 KB, free 333.3 KB)
17/08/11 07:03:37 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.2:51832 (size: 6.2 KB, free: 511.1 MB)
17/08/11 07:03:37 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/08/11 07:03:37 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at map at ProfileCore.scala:36)
17/08/11 07:03:37 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks
17/08/11 07:03:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, sandbox, partition 0,NODE_LOCAL, 2372 bytes)
17/08/11 07:03:39 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on sandbox:42164 (size: 6.2 KB, free: 511.1 MB)
17/08/11 07:03:40 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on sandbox:42164 (size: 19.6 KB, free: 511.1 MB)
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, sandbox, partition 1,NODE_LOCAL, 2372 bytes)
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4474 ms on sandbox (1/2)
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 262 ms on sandbox (2/2)
17/08/11 07:03:42 INFO scheduler.DAGScheduler: ResultStage 0 (count at ProfileCore.scala:42) finished in 4.720 s
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Job 0 finished: count at ProfileCore.scala:42, took 4.825490 s
17/08/11 07:03:42 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/08/11 07:03:42 INFO spark.SparkContext: Starting job: count at ProfileCore.scala:44
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Got job 1 (count at ProfileCore.scala:44) with 2 output partitions
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at ProfileCore.scala:44)
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Parents of final stage: List()
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at map at ProfileCore.scala:43), which has no missing parents
17/08/11 07:03:42 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.6 KB, free 346.9 KB)
17/08/11 07:03:42 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 353.2 KB)
17/08/11 07:03:42 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.2:51832 (size: 6.3 KB, free: 511.1 MB)
17/08/11 07:03:42 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at map at ProfileCore.scala:43)
17/08/11 07:03:42 INFO cluster.YarnScheduler: Adding task set 1.0 with 2 tasks
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, sandbox, partition 0,NODE_LOCAL, 2372 bytes)
17/08/11 07:03:42 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on sandbox:42164 (size: 6.3 KB, free: 511.1 MB)
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, sandbox, partition 1,NODE_LOCAL, 2372 bytes)
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 205 ms on sandbox (1/2)
17/08/11 07:03:42 INFO scheduler.DAGScheduler: ResultStage 1 (count at ProfileCore.scala:44) finished in 0.365 s
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Job 1 finished: count at ProfileCore.scala:44, took 0.380093 s
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 165 ms on sandbox (2/2)
17/08/11 07:03:42 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/08/11 07:03:42 INFO spark.SparkContext: Starting job: count at ProfileCore.scala:46
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Got job 2 (count at ProfileCore.scala:46) with 2 output partitions
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (count at ProfileCore.scala:46)
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Parents of final stage: List()
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Missing parents: List()
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at map at ProfileCore.scala:45), which has no missing parents
17/08/11 07:03:42 INFO storage.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.6 KB, free 366.8 KB)
17/08/11 07:03:42 INFO storage.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.3 KB, free 373.1 KB)
17/08/11 07:03:42 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.2:51832 (size: 6.3 KB, free: 511.1 MB)
17/08/11 07:03:42 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/08/11 07:03:42 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at map at ProfileCore.scala:45)
17/08/11 07:03:42 INFO cluster.YarnScheduler: Adding task set 2.0 with 2 tasks
17/08/11 07:03:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, sandbox, partition 0,NODE_LOCAL, 2372 bytes)
17/08/11 07:03:42 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on sandbox:42164 (size: 6.3 KB, free: 511.1 MB)
/root/docker.sh: line 17: 12442 Killed                  spark-submit --class com.griffin.profile.ProfileApplication --master yarn-client --num-executors 1 --conf "spark.yarn.dist.files=$SPARK_HOME/conf/hive-site.xml" griffin-profile-1.0-SNAPSHOT-jar-with-dependencies.jar ./enumProfile.json
